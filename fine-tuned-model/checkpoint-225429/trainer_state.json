{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9999800381935895,
  "eval_steps": 500,
  "global_step": 225429,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006653935470133811,
      "grad_norm": 33.13024139404297,
      "learning_rate": 4.9889100337578574e-05,
      "loss": 7.5695,
      "step": 500
    },
    {
      "epoch": 0.013307870940267622,
      "grad_norm": 11.951622009277344,
      "learning_rate": 4.977820067515715e-05,
      "loss": 6.6184,
      "step": 1000
    },
    {
      "epoch": 0.01996180641040143,
      "grad_norm": 32.15718460083008,
      "learning_rate": 4.966730101273572e-05,
      "loss": 6.5591,
      "step": 1500
    },
    {
      "epoch": 0.026615741880535244,
      "grad_norm": 12.393837928771973,
      "learning_rate": 4.955640135031429e-05,
      "loss": 5.9262,
      "step": 2000
    },
    {
      "epoch": 0.033269677350669054,
      "grad_norm": 7.884124755859375,
      "learning_rate": 4.944550168789287e-05,
      "loss": 5.6984,
      "step": 2500
    },
    {
      "epoch": 0.03992361282080286,
      "grad_norm": 7.6311869621276855,
      "learning_rate": 4.933460202547144e-05,
      "loss": 6.0253,
      "step": 3000
    },
    {
      "epoch": 0.046577548290936674,
      "grad_norm": 0.0,
      "learning_rate": 4.922370236305001e-05,
      "loss": 6.1733,
      "step": 3500
    },
    {
      "epoch": 0.05323148376107049,
      "grad_norm": 10.83934211730957,
      "learning_rate": 4.911280270062858e-05,
      "loss": 5.5341,
      "step": 4000
    },
    {
      "epoch": 0.059885419231204294,
      "grad_norm": 7.366620063781738,
      "learning_rate": 4.900190303820715e-05,
      "loss": 6.0258,
      "step": 4500
    },
    {
      "epoch": 0.06653935470133811,
      "grad_norm": 17.168163299560547,
      "learning_rate": 4.8891003375785725e-05,
      "loss": 5.5738,
      "step": 5000
    },
    {
      "epoch": 0.07319329017147191,
      "grad_norm": 11.861125946044922,
      "learning_rate": 4.87801037133643e-05,
      "loss": 5.5959,
      "step": 5500
    },
    {
      "epoch": 0.07984722564160572,
      "grad_norm": 0.0,
      "learning_rate": 4.8669204050942875e-05,
      "loss": 5.653,
      "step": 6000
    },
    {
      "epoch": 0.08650116111173954,
      "grad_norm": 30.221712112426758,
      "learning_rate": 4.855830438852144e-05,
      "loss": 5.6769,
      "step": 6500
    },
    {
      "epoch": 0.09315509658187335,
      "grad_norm": 20.58767318725586,
      "learning_rate": 4.844740472610002e-05,
      "loss": 5.6235,
      "step": 7000
    },
    {
      "epoch": 0.09980903205200715,
      "grad_norm": 12.459294319152832,
      "learning_rate": 4.833650506367859e-05,
      "loss": 5.6393,
      "step": 7500
    },
    {
      "epoch": 0.10646296752214097,
      "grad_norm": 15.910523414611816,
      "learning_rate": 4.822560540125716e-05,
      "loss": 5.2976,
      "step": 8000
    },
    {
      "epoch": 0.11311690299227478,
      "grad_norm": 18.11432647705078,
      "learning_rate": 4.811470573883574e-05,
      "loss": 5.4104,
      "step": 8500
    },
    {
      "epoch": 0.11977083846240859,
      "grad_norm": 3.6956377029418945,
      "learning_rate": 4.8003806076414304e-05,
      "loss": 5.7187,
      "step": 9000
    },
    {
      "epoch": 0.1264247739325424,
      "grad_norm": 5.109971523284912,
      "learning_rate": 4.7892906413992876e-05,
      "loss": 5.344,
      "step": 9500
    },
    {
      "epoch": 0.13307870940267622,
      "grad_norm": 17.11069679260254,
      "learning_rate": 4.7782006751571454e-05,
      "loss": 5.5142,
      "step": 10000
    },
    {
      "epoch": 0.13973264487281004,
      "grad_norm": 0.0,
      "learning_rate": 4.7671107089150025e-05,
      "loss": 5.4064,
      "step": 10500
    },
    {
      "epoch": 0.14638658034294383,
      "grad_norm": 10.569842338562012,
      "learning_rate": 4.75602074267286e-05,
      "loss": 5.7734,
      "step": 11000
    },
    {
      "epoch": 0.15304051581307765,
      "grad_norm": 9.0302734375,
      "learning_rate": 4.744930776430717e-05,
      "loss": 5.3152,
      "step": 11500
    },
    {
      "epoch": 0.15969445128321144,
      "grad_norm": 14.961248397827148,
      "learning_rate": 4.733840810188574e-05,
      "loss": 5.2015,
      "step": 12000
    },
    {
      "epoch": 0.16634838675334526,
      "grad_norm": 13.009613037109375,
      "learning_rate": 4.722750843946431e-05,
      "loss": 5.1726,
      "step": 12500
    },
    {
      "epoch": 0.17300232222347908,
      "grad_norm": 5.4060959815979,
      "learning_rate": 4.711660877704288e-05,
      "loss": 5.6003,
      "step": 13000
    },
    {
      "epoch": 0.17965625769361288,
      "grad_norm": 20.33284568786621,
      "learning_rate": 4.700570911462146e-05,
      "loss": 5.6137,
      "step": 13500
    },
    {
      "epoch": 0.1863101931637467,
      "grad_norm": 0.0,
      "learning_rate": 4.6894809452200026e-05,
      "loss": 5.1887,
      "step": 14000
    },
    {
      "epoch": 0.19296412863388052,
      "grad_norm": 5.872967720031738,
      "learning_rate": 4.6783909789778605e-05,
      "loss": 5.5156,
      "step": 14500
    },
    {
      "epoch": 0.1996180641040143,
      "grad_norm": 4.1763763427734375,
      "learning_rate": 4.6673010127357176e-05,
      "loss": 5.2494,
      "step": 15000
    },
    {
      "epoch": 0.20627199957414813,
      "grad_norm": 12.338495254516602,
      "learning_rate": 4.656211046493575e-05,
      "loss": 4.9351,
      "step": 15500
    },
    {
      "epoch": 0.21292593504428195,
      "grad_norm": 6.852132797241211,
      "learning_rate": 4.645121080251432e-05,
      "loss": 5.2739,
      "step": 16000
    },
    {
      "epoch": 0.21957987051441574,
      "grad_norm": 6.504515171051025,
      "learning_rate": 4.634031114009289e-05,
      "loss": 5.0682,
      "step": 16500
    },
    {
      "epoch": 0.22623380598454956,
      "grad_norm": 5.878200054168701,
      "learning_rate": 4.622941147767146e-05,
      "loss": 5.0351,
      "step": 17000
    },
    {
      "epoch": 0.23288774145468338,
      "grad_norm": 11.663487434387207,
      "learning_rate": 4.6118511815250034e-05,
      "loss": 5.575,
      "step": 17500
    },
    {
      "epoch": 0.23954167692481718,
      "grad_norm": 5.464523792266846,
      "learning_rate": 4.600761215282861e-05,
      "loss": 5.3513,
      "step": 18000
    },
    {
      "epoch": 0.246195612394951,
      "grad_norm": 15.746757507324219,
      "learning_rate": 4.589671249040718e-05,
      "loss": 4.9149,
      "step": 18500
    },
    {
      "epoch": 0.2528495478650848,
      "grad_norm": 4.7400946617126465,
      "learning_rate": 4.5785812827985755e-05,
      "loss": 4.8731,
      "step": 19000
    },
    {
      "epoch": 0.2595034833352186,
      "grad_norm": 5.99653959274292,
      "learning_rate": 4.567491316556433e-05,
      "loss": 5.3275,
      "step": 19500
    },
    {
      "epoch": 0.26615741880535243,
      "grad_norm": 9.805676460266113,
      "learning_rate": 4.55640135031429e-05,
      "loss": 4.8015,
      "step": 20000
    },
    {
      "epoch": 0.27281135427548625,
      "grad_norm": 4.741814613342285,
      "learning_rate": 4.545311384072147e-05,
      "loss": 4.994,
      "step": 20500
    },
    {
      "epoch": 0.27946528974562007,
      "grad_norm": 0.0,
      "learning_rate": 4.534221417830004e-05,
      "loss": 4.8964,
      "step": 21000
    },
    {
      "epoch": 0.28611922521575384,
      "grad_norm": 3.715907096862793,
      "learning_rate": 4.523131451587861e-05,
      "loss": 4.9791,
      "step": 21500
    },
    {
      "epoch": 0.29277316068588766,
      "grad_norm": 25.64331817626953,
      "learning_rate": 4.5120414853457185e-05,
      "loss": 4.5875,
      "step": 22000
    },
    {
      "epoch": 0.2994270961560215,
      "grad_norm": 8.785085678100586,
      "learning_rate": 4.500951519103576e-05,
      "loss": 5.3827,
      "step": 22500
    },
    {
      "epoch": 0.3060810316261553,
      "grad_norm": 6.782907962799072,
      "learning_rate": 4.4898615528614335e-05,
      "loss": 4.8455,
      "step": 23000
    },
    {
      "epoch": 0.3127349670962891,
      "grad_norm": 5.687613010406494,
      "learning_rate": 4.4787715866192906e-05,
      "loss": 5.0712,
      "step": 23500
    },
    {
      "epoch": 0.3193889025664229,
      "grad_norm": 4.363492488861084,
      "learning_rate": 4.467681620377148e-05,
      "loss": 4.7122,
      "step": 24000
    },
    {
      "epoch": 0.3260428380365567,
      "grad_norm": 8.746650695800781,
      "learning_rate": 4.456591654135005e-05,
      "loss": 5.2854,
      "step": 24500
    },
    {
      "epoch": 0.3326967735066905,
      "grad_norm": 5.390724182128906,
      "learning_rate": 4.445501687892862e-05,
      "loss": 5.2444,
      "step": 25000
    },
    {
      "epoch": 0.33935070897682434,
      "grad_norm": 10.057943344116211,
      "learning_rate": 4.43441172165072e-05,
      "loss": 5.1186,
      "step": 25500
    },
    {
      "epoch": 0.34600464444695817,
      "grad_norm": 12.507247924804688,
      "learning_rate": 4.4233217554085764e-05,
      "loss": 5.3164,
      "step": 26000
    },
    {
      "epoch": 0.352658579917092,
      "grad_norm": 14.984611511230469,
      "learning_rate": 4.4122317891664336e-05,
      "loss": 5.1595,
      "step": 26500
    },
    {
      "epoch": 0.35931251538722575,
      "grad_norm": 2.661848783493042,
      "learning_rate": 4.4011418229242914e-05,
      "loss": 4.9066,
      "step": 27000
    },
    {
      "epoch": 0.36596645085735957,
      "grad_norm": 6.505558490753174,
      "learning_rate": 4.3900518566821485e-05,
      "loss": 4.8438,
      "step": 27500
    },
    {
      "epoch": 0.3726203863274934,
      "grad_norm": 7.571971416473389,
      "learning_rate": 4.378961890440006e-05,
      "loss": 4.9829,
      "step": 28000
    },
    {
      "epoch": 0.3792743217976272,
      "grad_norm": 9.58385944366455,
      "learning_rate": 4.367871924197863e-05,
      "loss": 5.4591,
      "step": 28500
    },
    {
      "epoch": 0.38592825726776103,
      "grad_norm": 11.772156715393066,
      "learning_rate": 4.35678195795572e-05,
      "loss": 4.6829,
      "step": 29000
    },
    {
      "epoch": 0.39258219273789485,
      "grad_norm": 12.923713684082031,
      "learning_rate": 4.345691991713577e-05,
      "loss": 5.1656,
      "step": 29500
    },
    {
      "epoch": 0.3992361282080286,
      "grad_norm": 8.618961334228516,
      "learning_rate": 4.334602025471435e-05,
      "loss": 5.2877,
      "step": 30000
    },
    {
      "epoch": 0.40589006367816244,
      "grad_norm": 6.577220439910889,
      "learning_rate": 4.323512059229292e-05,
      "loss": 5.0061,
      "step": 30500
    },
    {
      "epoch": 0.41254399914829626,
      "grad_norm": 12.1964750289917,
      "learning_rate": 4.3124220929871486e-05,
      "loss": 4.6839,
      "step": 31000
    },
    {
      "epoch": 0.4191979346184301,
      "grad_norm": 4.1544952392578125,
      "learning_rate": 4.3013321267450065e-05,
      "loss": 5.0017,
      "step": 31500
    },
    {
      "epoch": 0.4258518700885639,
      "grad_norm": 14.580862998962402,
      "learning_rate": 4.2902421605028636e-05,
      "loss": 5.0724,
      "step": 32000
    },
    {
      "epoch": 0.43250580555869766,
      "grad_norm": 4.733335018157959,
      "learning_rate": 4.279152194260721e-05,
      "loss": 5.1934,
      "step": 32500
    },
    {
      "epoch": 0.4391597410288315,
      "grad_norm": 0.0,
      "learning_rate": 4.2680622280185786e-05,
      "loss": 4.8911,
      "step": 33000
    },
    {
      "epoch": 0.4458136764989653,
      "grad_norm": 4.265070915222168,
      "learning_rate": 4.256972261776435e-05,
      "loss": 5.209,
      "step": 33500
    },
    {
      "epoch": 0.4524676119690991,
      "grad_norm": 15.189756393432617,
      "learning_rate": 4.245882295534292e-05,
      "loss": 4.8244,
      "step": 34000
    },
    {
      "epoch": 0.45912154743923295,
      "grad_norm": 10.17501163482666,
      "learning_rate": 4.23479232929215e-05,
      "loss": 5.1236,
      "step": 34500
    },
    {
      "epoch": 0.46577548290936677,
      "grad_norm": 8.812520980834961,
      "learning_rate": 4.223702363050007e-05,
      "loss": 4.6405,
      "step": 35000
    },
    {
      "epoch": 0.47242941837950053,
      "grad_norm": 5.944868564605713,
      "learning_rate": 4.2126123968078644e-05,
      "loss": 4.9153,
      "step": 35500
    },
    {
      "epoch": 0.47908335384963435,
      "grad_norm": 5.657558441162109,
      "learning_rate": 4.2015224305657215e-05,
      "loss": 4.9397,
      "step": 36000
    },
    {
      "epoch": 0.4857372893197682,
      "grad_norm": 7.80435848236084,
      "learning_rate": 4.190432464323579e-05,
      "loss": 4.8268,
      "step": 36500
    },
    {
      "epoch": 0.492391224789902,
      "grad_norm": 5.963541507720947,
      "learning_rate": 4.179342498081436e-05,
      "loss": 5.0728,
      "step": 37000
    },
    {
      "epoch": 0.4990451602600358,
      "grad_norm": 5.983006477355957,
      "learning_rate": 4.168252531839294e-05,
      "loss": 4.909,
      "step": 37500
    },
    {
      "epoch": 0.5056990957301696,
      "grad_norm": 7.85465669631958,
      "learning_rate": 4.157162565597151e-05,
      "loss": 5.1598,
      "step": 38000
    },
    {
      "epoch": 0.5123530312003034,
      "grad_norm": 5.879385948181152,
      "learning_rate": 4.146072599355007e-05,
      "loss": 4.9718,
      "step": 38500
    },
    {
      "epoch": 0.5190069666704372,
      "grad_norm": 9.95192813873291,
      "learning_rate": 4.134982633112865e-05,
      "loss": 4.8486,
      "step": 39000
    },
    {
      "epoch": 0.525660902140571,
      "grad_norm": 13.57669734954834,
      "learning_rate": 4.123892666870722e-05,
      "loss": 5.2267,
      "step": 39500
    },
    {
      "epoch": 0.5323148376107049,
      "grad_norm": 5.887800693511963,
      "learning_rate": 4.1128027006285795e-05,
      "loss": 5.0237,
      "step": 40000
    },
    {
      "epoch": 0.5389687730808387,
      "grad_norm": 4.263962745666504,
      "learning_rate": 4.101712734386437e-05,
      "loss": 4.7781,
      "step": 40500
    },
    {
      "epoch": 0.5456227085509725,
      "grad_norm": 0.0,
      "learning_rate": 4.090622768144294e-05,
      "loss": 5.0834,
      "step": 41000
    },
    {
      "epoch": 0.5522766440211063,
      "grad_norm": 14.104475021362305,
      "learning_rate": 4.079532801902151e-05,
      "loss": 4.7988,
      "step": 41500
    },
    {
      "epoch": 0.5589305794912401,
      "grad_norm": 7.580873012542725,
      "learning_rate": 4.068442835660009e-05,
      "loss": 4.5343,
      "step": 42000
    },
    {
      "epoch": 0.5655845149613739,
      "grad_norm": 3.684469699859619,
      "learning_rate": 4.057352869417866e-05,
      "loss": 4.7895,
      "step": 42500
    },
    {
      "epoch": 0.5722384504315077,
      "grad_norm": 7.129920959472656,
      "learning_rate": 4.0462629031757224e-05,
      "loss": 4.982,
      "step": 43000
    },
    {
      "epoch": 0.5788923859016415,
      "grad_norm": 10.940680503845215,
      "learning_rate": 4.03517293693358e-05,
      "loss": 4.7169,
      "step": 43500
    },
    {
      "epoch": 0.5855463213717753,
      "grad_norm": 24.293962478637695,
      "learning_rate": 4.0240829706914374e-05,
      "loss": 5.1992,
      "step": 44000
    },
    {
      "epoch": 0.5922002568419091,
      "grad_norm": 3.7610859870910645,
      "learning_rate": 4.0129930044492945e-05,
      "loss": 5.3842,
      "step": 44500
    },
    {
      "epoch": 0.598854192312043,
      "grad_norm": 5.80797815322876,
      "learning_rate": 4.0019030382071524e-05,
      "loss": 4.8768,
      "step": 45000
    },
    {
      "epoch": 0.6055081277821768,
      "grad_norm": 0.0,
      "learning_rate": 3.990813071965009e-05,
      "loss": 4.5959,
      "step": 45500
    },
    {
      "epoch": 0.6121620632523106,
      "grad_norm": 6.651771068572998,
      "learning_rate": 3.979723105722866e-05,
      "loss": 5.0052,
      "step": 46000
    },
    {
      "epoch": 0.6188159987224444,
      "grad_norm": 0.0,
      "learning_rate": 3.968633139480724e-05,
      "loss": 4.5353,
      "step": 46500
    },
    {
      "epoch": 0.6254699341925782,
      "grad_norm": 12.156085014343262,
      "learning_rate": 3.957543173238581e-05,
      "loss": 4.916,
      "step": 47000
    },
    {
      "epoch": 0.6321238696627121,
      "grad_norm": 0.902265727519989,
      "learning_rate": 3.946453206996438e-05,
      "loss": 4.7607,
      "step": 47500
    },
    {
      "epoch": 0.6387778051328458,
      "grad_norm": 18.27424430847168,
      "learning_rate": 3.935363240754295e-05,
      "loss": 4.3454,
      "step": 48000
    },
    {
      "epoch": 0.6454317406029796,
      "grad_norm": 10.118738174438477,
      "learning_rate": 3.9242732745121525e-05,
      "loss": 4.9315,
      "step": 48500
    },
    {
      "epoch": 0.6520856760731134,
      "grad_norm": 5.173923015594482,
      "learning_rate": 3.9131833082700096e-05,
      "loss": 4.8818,
      "step": 49000
    },
    {
      "epoch": 0.6587396115432472,
      "grad_norm": 8.981586456298828,
      "learning_rate": 3.9020933420278675e-05,
      "loss": 4.9706,
      "step": 49500
    },
    {
      "epoch": 0.665393547013381,
      "grad_norm": 13.498817443847656,
      "learning_rate": 3.8910033757857246e-05,
      "loss": 4.7941,
      "step": 50000
    },
    {
      "epoch": 0.6720474824835149,
      "grad_norm": 0.4676799774169922,
      "learning_rate": 3.879913409543581e-05,
      "loss": 5.2415,
      "step": 50500
    },
    {
      "epoch": 0.6787014179536487,
      "grad_norm": 20.069368362426758,
      "learning_rate": 3.868823443301439e-05,
      "loss": 4.6483,
      "step": 51000
    },
    {
      "epoch": 0.6853553534237825,
      "grad_norm": 11.645145416259766,
      "learning_rate": 3.857733477059296e-05,
      "loss": 4.7574,
      "step": 51500
    },
    {
      "epoch": 0.6920092888939163,
      "grad_norm": 0.1381302922964096,
      "learning_rate": 3.846643510817153e-05,
      "loss": 4.6756,
      "step": 52000
    },
    {
      "epoch": 0.6986632243640502,
      "grad_norm": 5.496386528015137,
      "learning_rate": 3.8355535445750104e-05,
      "loss": 4.7557,
      "step": 52500
    },
    {
      "epoch": 0.705317159834184,
      "grad_norm": 5.456035137176514,
      "learning_rate": 3.8244635783328675e-05,
      "loss": 4.5352,
      "step": 53000
    },
    {
      "epoch": 0.7119710953043178,
      "grad_norm": 7.335542678833008,
      "learning_rate": 3.813373612090725e-05,
      "loss": 4.8851,
      "step": 53500
    },
    {
      "epoch": 0.7186250307744515,
      "grad_norm": 13.137839317321777,
      "learning_rate": 3.8022836458485825e-05,
      "loss": 5.0861,
      "step": 54000
    },
    {
      "epoch": 0.7252789662445853,
      "grad_norm": 12.27609634399414,
      "learning_rate": 3.79119367960644e-05,
      "loss": 4.6457,
      "step": 54500
    },
    {
      "epoch": 0.7319329017147191,
      "grad_norm": 9.473834991455078,
      "learning_rate": 3.780103713364297e-05,
      "loss": 4.78,
      "step": 55000
    },
    {
      "epoch": 0.738586837184853,
      "grad_norm": 10.573945999145508,
      "learning_rate": 3.769013747122154e-05,
      "loss": 4.8652,
      "step": 55500
    },
    {
      "epoch": 0.7452407726549868,
      "grad_norm": 6.9988484382629395,
      "learning_rate": 3.757923780880011e-05,
      "loss": 4.7557,
      "step": 56000
    },
    {
      "epoch": 0.7518947081251206,
      "grad_norm": 4.17983865737915,
      "learning_rate": 3.746833814637868e-05,
      "loss": 4.7733,
      "step": 56500
    },
    {
      "epoch": 0.7585486435952544,
      "grad_norm": 6.613187313079834,
      "learning_rate": 3.7357438483957255e-05,
      "loss": 4.9792,
      "step": 57000
    },
    {
      "epoch": 0.7652025790653882,
      "grad_norm": 15.305123329162598,
      "learning_rate": 3.724653882153583e-05,
      "loss": 4.6802,
      "step": 57500
    },
    {
      "epoch": 0.7718565145355221,
      "grad_norm": 11.733426094055176,
      "learning_rate": 3.71356391591144e-05,
      "loss": 4.5181,
      "step": 58000
    },
    {
      "epoch": 0.7785104500056559,
      "grad_norm": 12.920053482055664,
      "learning_rate": 3.7024739496692976e-05,
      "loss": 4.543,
      "step": 58500
    },
    {
      "epoch": 0.7851643854757897,
      "grad_norm": 8.769601821899414,
      "learning_rate": 3.691383983427155e-05,
      "loss": 4.7407,
      "step": 59000
    },
    {
      "epoch": 0.7918183209459234,
      "grad_norm": 9.163586616516113,
      "learning_rate": 3.680294017185012e-05,
      "loss": 4.943,
      "step": 59500
    },
    {
      "epoch": 0.7984722564160572,
      "grad_norm": 9.398103713989258,
      "learning_rate": 3.669204050942869e-05,
      "loss": 5.0716,
      "step": 60000
    },
    {
      "epoch": 0.8051261918861911,
      "grad_norm": 17.435354232788086,
      "learning_rate": 3.658114084700726e-05,
      "loss": 4.7238,
      "step": 60500
    },
    {
      "epoch": 0.8117801273563249,
      "grad_norm": 3.5731282234191895,
      "learning_rate": 3.6470241184585834e-05,
      "loss": 4.9442,
      "step": 61000
    },
    {
      "epoch": 0.8184340628264587,
      "grad_norm": 4.353721618652344,
      "learning_rate": 3.6359341522164406e-05,
      "loss": 5.0331,
      "step": 61500
    },
    {
      "epoch": 0.8250879982965925,
      "grad_norm": 10.013397216796875,
      "learning_rate": 3.6248441859742984e-05,
      "loss": 4.8409,
      "step": 62000
    },
    {
      "epoch": 0.8317419337667263,
      "grad_norm": 8.44990348815918,
      "learning_rate": 3.6137542197321555e-05,
      "loss": 4.7678,
      "step": 62500
    },
    {
      "epoch": 0.8383958692368602,
      "grad_norm": 13.645322799682617,
      "learning_rate": 3.602664253490012e-05,
      "loss": 4.589,
      "step": 63000
    },
    {
      "epoch": 0.845049804706994,
      "grad_norm": 14.909855842590332,
      "learning_rate": 3.59157428724787e-05,
      "loss": 4.8042,
      "step": 63500
    },
    {
      "epoch": 0.8517037401771278,
      "grad_norm": 18.54168701171875,
      "learning_rate": 3.580484321005727e-05,
      "loss": 4.5444,
      "step": 64000
    },
    {
      "epoch": 0.8583576756472616,
      "grad_norm": 6.3428144454956055,
      "learning_rate": 3.569394354763584e-05,
      "loss": 4.82,
      "step": 64500
    },
    {
      "epoch": 0.8650116111173953,
      "grad_norm": 12.195958137512207,
      "learning_rate": 3.558304388521442e-05,
      "loss": 4.8615,
      "step": 65000
    },
    {
      "epoch": 0.8716655465875291,
      "grad_norm": 4.55608606338501,
      "learning_rate": 3.5472144222792985e-05,
      "loss": 4.9711,
      "step": 65500
    },
    {
      "epoch": 0.878319482057663,
      "grad_norm": 8.259613037109375,
      "learning_rate": 3.5361244560371556e-05,
      "loss": 4.6403,
      "step": 66000
    },
    {
      "epoch": 0.8849734175277968,
      "grad_norm": 4.116787910461426,
      "learning_rate": 3.5250344897950135e-05,
      "loss": 4.6913,
      "step": 66500
    },
    {
      "epoch": 0.8916273529979306,
      "grad_norm": 7.950854301452637,
      "learning_rate": 3.5139445235528706e-05,
      "loss": 4.3836,
      "step": 67000
    },
    {
      "epoch": 0.8982812884680644,
      "grad_norm": 15.929657936096191,
      "learning_rate": 3.502854557310728e-05,
      "loss": 4.7754,
      "step": 67500
    },
    {
      "epoch": 0.9049352239381983,
      "grad_norm": 6.128769874572754,
      "learning_rate": 3.491764591068585e-05,
      "loss": 4.7891,
      "step": 68000
    },
    {
      "epoch": 0.9115891594083321,
      "grad_norm": 34.513370513916016,
      "learning_rate": 3.480674624826442e-05,
      "loss": 4.7299,
      "step": 68500
    },
    {
      "epoch": 0.9182430948784659,
      "grad_norm": 5.4930315017700195,
      "learning_rate": 3.469584658584299e-05,
      "loss": 5.3183,
      "step": 69000
    },
    {
      "epoch": 0.9248970303485997,
      "grad_norm": 4.4665679931640625,
      "learning_rate": 3.458494692342157e-05,
      "loss": 4.5069,
      "step": 69500
    },
    {
      "epoch": 0.9315509658187335,
      "grad_norm": 10.086212158203125,
      "learning_rate": 3.447404726100014e-05,
      "loss": 4.8108,
      "step": 70000
    },
    {
      "epoch": 0.9382049012888674,
      "grad_norm": 9.816573143005371,
      "learning_rate": 3.436314759857871e-05,
      "loss": 4.5716,
      "step": 70500
    },
    {
      "epoch": 0.9448588367590011,
      "grad_norm": 9.775300979614258,
      "learning_rate": 3.4252247936157285e-05,
      "loss": 4.7851,
      "step": 71000
    },
    {
      "epoch": 0.9515127722291349,
      "grad_norm": 3.0994346141815186,
      "learning_rate": 3.414134827373586e-05,
      "loss": 4.4966,
      "step": 71500
    },
    {
      "epoch": 0.9581667076992687,
      "grad_norm": 14.9404935836792,
      "learning_rate": 3.403044861131443e-05,
      "loss": 4.9645,
      "step": 72000
    },
    {
      "epoch": 0.9648206431694025,
      "grad_norm": 7.113792896270752,
      "learning_rate": 3.3919548948893e-05,
      "loss": 4.5176,
      "step": 72500
    },
    {
      "epoch": 0.9714745786395363,
      "grad_norm": 13.447235107421875,
      "learning_rate": 3.380864928647157e-05,
      "loss": 4.5125,
      "step": 73000
    },
    {
      "epoch": 0.9781285141096702,
      "grad_norm": 21.494352340698242,
      "learning_rate": 3.369774962405014e-05,
      "loss": 4.7887,
      "step": 73500
    },
    {
      "epoch": 0.984782449579804,
      "grad_norm": 0.0,
      "learning_rate": 3.358684996162872e-05,
      "loss": 4.8232,
      "step": 74000
    },
    {
      "epoch": 0.9914363850499378,
      "grad_norm": 4.885494232177734,
      "learning_rate": 3.347595029920729e-05,
      "loss": 4.5949,
      "step": 74500
    },
    {
      "epoch": 0.9980903205200716,
      "grad_norm": 6.813508987426758,
      "learning_rate": 3.336505063678586e-05,
      "loss": 4.6018,
      "step": 75000
    },
    {
      "epoch": 1.0047442559902053,
      "grad_norm": 7.242222309112549,
      "learning_rate": 3.3254150974364436e-05,
      "loss": 4.5998,
      "step": 75500
    },
    {
      "epoch": 1.0113981914603392,
      "grad_norm": 5.469526290893555,
      "learning_rate": 3.314325131194301e-05,
      "loss": 4.4978,
      "step": 76000
    },
    {
      "epoch": 1.018052126930473,
      "grad_norm": 8.592080116271973,
      "learning_rate": 3.303235164952158e-05,
      "loss": 4.3664,
      "step": 76500
    },
    {
      "epoch": 1.0247060624006068,
      "grad_norm": 7.476579189300537,
      "learning_rate": 3.292145198710016e-05,
      "loss": 4.2629,
      "step": 77000
    },
    {
      "epoch": 1.0313599978707406,
      "grad_norm": 0.2553151547908783,
      "learning_rate": 3.281055232467872e-05,
      "loss": 3.9345,
      "step": 77500
    },
    {
      "epoch": 1.0380139333408744,
      "grad_norm": 6.0049567222595215,
      "learning_rate": 3.2699652662257294e-05,
      "loss": 4.3358,
      "step": 78000
    },
    {
      "epoch": 1.0446678688110083,
      "grad_norm": 18.6414737701416,
      "learning_rate": 3.258875299983587e-05,
      "loss": 4.2473,
      "step": 78500
    },
    {
      "epoch": 1.051321804281142,
      "grad_norm": 10.33421516418457,
      "learning_rate": 3.2477853337414444e-05,
      "loss": 4.2148,
      "step": 79000
    },
    {
      "epoch": 1.057975739751276,
      "grad_norm": 17.575042724609375,
      "learning_rate": 3.2366953674993015e-05,
      "loss": 4.0528,
      "step": 79500
    },
    {
      "epoch": 1.0646296752214097,
      "grad_norm": 0.02963666431605816,
      "learning_rate": 3.225605401257159e-05,
      "loss": 3.9887,
      "step": 80000
    },
    {
      "epoch": 1.0712836106915435,
      "grad_norm": 9.452156066894531,
      "learning_rate": 3.214515435015016e-05,
      "loss": 4.2397,
      "step": 80500
    },
    {
      "epoch": 1.0779375461616774,
      "grad_norm": 5.95436954498291,
      "learning_rate": 3.203425468772873e-05,
      "loss": 4.234,
      "step": 81000
    },
    {
      "epoch": 1.0845914816318112,
      "grad_norm": 0.10839349776506424,
      "learning_rate": 3.192335502530731e-05,
      "loss": 4.2096,
      "step": 81500
    },
    {
      "epoch": 1.091245417101945,
      "grad_norm": 7.056090354919434,
      "learning_rate": 3.181245536288588e-05,
      "loss": 4.2002,
      "step": 82000
    },
    {
      "epoch": 1.0978993525720788,
      "grad_norm": 17.81836700439453,
      "learning_rate": 3.1701555700464445e-05,
      "loss": 4.2107,
      "step": 82500
    },
    {
      "epoch": 1.1045532880422126,
      "grad_norm": 5.929683685302734,
      "learning_rate": 3.159065603804302e-05,
      "loss": 4.2589,
      "step": 83000
    },
    {
      "epoch": 1.1112072235123465,
      "grad_norm": 10.878419876098633,
      "learning_rate": 3.1479756375621595e-05,
      "loss": 3.9599,
      "step": 83500
    },
    {
      "epoch": 1.1178611589824803,
      "grad_norm": 14.343668937683105,
      "learning_rate": 3.1368856713200166e-05,
      "loss": 4.1776,
      "step": 84000
    },
    {
      "epoch": 1.1245150944526139,
      "grad_norm": 18.763111114501953,
      "learning_rate": 3.125795705077874e-05,
      "loss": 4.3104,
      "step": 84500
    },
    {
      "epoch": 1.1311690299227477,
      "grad_norm": 18.574525833129883,
      "learning_rate": 3.114705738835731e-05,
      "loss": 4.0804,
      "step": 85000
    },
    {
      "epoch": 1.1378229653928815,
      "grad_norm": 11.191585540771484,
      "learning_rate": 3.103615772593588e-05,
      "loss": 4.16,
      "step": 85500
    },
    {
      "epoch": 1.1444769008630153,
      "grad_norm": 4.990921497344971,
      "learning_rate": 3.092525806351446e-05,
      "loss": 4.4533,
      "step": 86000
    },
    {
      "epoch": 1.1511308363331492,
      "grad_norm": 8.77486515045166,
      "learning_rate": 3.081435840109303e-05,
      "loss": 4.0168,
      "step": 86500
    },
    {
      "epoch": 1.157784771803283,
      "grad_norm": 0.08344539254903793,
      "learning_rate": 3.07034587386716e-05,
      "loss": 4.3831,
      "step": 87000
    },
    {
      "epoch": 1.1644387072734168,
      "grad_norm": 6.044975280761719,
      "learning_rate": 3.0592559076250174e-05,
      "loss": 4.3754,
      "step": 87500
    },
    {
      "epoch": 1.1710926427435506,
      "grad_norm": 5.182060718536377,
      "learning_rate": 3.0481659413828745e-05,
      "loss": 4.0337,
      "step": 88000
    },
    {
      "epoch": 1.1777465782136844,
      "grad_norm": 6.9948015213012695,
      "learning_rate": 3.0370759751407317e-05,
      "loss": 4.1596,
      "step": 88500
    },
    {
      "epoch": 1.1844005136838183,
      "grad_norm": 7.329315662384033,
      "learning_rate": 3.0259860088985892e-05,
      "loss": 4.262,
      "step": 89000
    },
    {
      "epoch": 1.191054449153952,
      "grad_norm": 13.209592819213867,
      "learning_rate": 3.0148960426564467e-05,
      "loss": 4.1366,
      "step": 89500
    },
    {
      "epoch": 1.197708384624086,
      "grad_norm": 0.3313980996608734,
      "learning_rate": 3.0038060764143035e-05,
      "loss": 4.2024,
      "step": 90000
    },
    {
      "epoch": 1.2043623200942197,
      "grad_norm": 7.659153461456299,
      "learning_rate": 2.9927161101721607e-05,
      "loss": 4.6627,
      "step": 90500
    },
    {
      "epoch": 1.2110162555643535,
      "grad_norm": 8.181818962097168,
      "learning_rate": 2.981626143930018e-05,
      "loss": 4.173,
      "step": 91000
    },
    {
      "epoch": 1.2176701910344874,
      "grad_norm": 5.472048759460449,
      "learning_rate": 2.9705361776878753e-05,
      "loss": 4.2548,
      "step": 91500
    },
    {
      "epoch": 1.2243241265046212,
      "grad_norm": 6.392847061157227,
      "learning_rate": 2.9594462114457328e-05,
      "loss": 4.1704,
      "step": 92000
    },
    {
      "epoch": 1.230978061974755,
      "grad_norm": 9.629734992980957,
      "learning_rate": 2.9483562452035896e-05,
      "loss": 4.2543,
      "step": 92500
    },
    {
      "epoch": 1.2376319974448888,
      "grad_norm": 0.0,
      "learning_rate": 2.9372662789614468e-05,
      "loss": 4.4368,
      "step": 93000
    },
    {
      "epoch": 1.2442859329150227,
      "grad_norm": 6.656480312347412,
      "learning_rate": 2.9261763127193043e-05,
      "loss": 4.2473,
      "step": 93500
    },
    {
      "epoch": 1.2509398683851565,
      "grad_norm": 14.599227905273438,
      "learning_rate": 2.9150863464771618e-05,
      "loss": 4.0996,
      "step": 94000
    },
    {
      "epoch": 1.2575938038552903,
      "grad_norm": 8.208314895629883,
      "learning_rate": 2.903996380235019e-05,
      "loss": 4.5562,
      "step": 94500
    },
    {
      "epoch": 1.2642477393254241,
      "grad_norm": 5.367828845977783,
      "learning_rate": 2.8929064139928757e-05,
      "loss": 4.1223,
      "step": 95000
    },
    {
      "epoch": 1.270901674795558,
      "grad_norm": 5.952755928039551,
      "learning_rate": 2.8818164477507332e-05,
      "loss": 4.1879,
      "step": 95500
    },
    {
      "epoch": 1.2775556102656918,
      "grad_norm": 8.668874740600586,
      "learning_rate": 2.8707264815085904e-05,
      "loss": 4.4651,
      "step": 96000
    },
    {
      "epoch": 1.2842095457358256,
      "grad_norm": 8.491652488708496,
      "learning_rate": 2.859636515266448e-05,
      "loss": 4.5764,
      "step": 96500
    },
    {
      "epoch": 1.2908634812059594,
      "grad_norm": 0.0,
      "learning_rate": 2.848546549024305e-05,
      "loss": 4.4819,
      "step": 97000
    },
    {
      "epoch": 1.297517416676093,
      "grad_norm": 0.0,
      "learning_rate": 2.837456582782162e-05,
      "loss": 4.3399,
      "step": 97500
    },
    {
      "epoch": 1.3041713521462268,
      "grad_norm": 9.402275085449219,
      "learning_rate": 2.8263666165400194e-05,
      "loss": 4.1169,
      "step": 98000
    },
    {
      "epoch": 1.3108252876163606,
      "grad_norm": 6.087335109710693,
      "learning_rate": 2.815276650297877e-05,
      "loss": 4.6312,
      "step": 98500
    },
    {
      "epoch": 1.3174792230864945,
      "grad_norm": 5.02224588394165,
      "learning_rate": 2.804186684055734e-05,
      "loss": 4.4509,
      "step": 99000
    },
    {
      "epoch": 1.3241331585566283,
      "grad_norm": 8.332634925842285,
      "learning_rate": 2.7930967178135908e-05,
      "loss": 4.197,
      "step": 99500
    },
    {
      "epoch": 1.330787094026762,
      "grad_norm": 14.900604248046875,
      "learning_rate": 2.7820067515714483e-05,
      "loss": 4.0547,
      "step": 100000
    },
    {
      "epoch": 1.337441029496896,
      "grad_norm": 7.677389621734619,
      "learning_rate": 2.7709167853293055e-05,
      "loss": 4.4023,
      "step": 100500
    },
    {
      "epoch": 1.3440949649670297,
      "grad_norm": 0.013302743434906006,
      "learning_rate": 2.759826819087163e-05,
      "loss": 4.0298,
      "step": 101000
    },
    {
      "epoch": 1.3507489004371636,
      "grad_norm": 7.044439315795898,
      "learning_rate": 2.74873685284502e-05,
      "loss": 4.3241,
      "step": 101500
    },
    {
      "epoch": 1.3574028359072974,
      "grad_norm": 0.029778819531202316,
      "learning_rate": 2.737646886602877e-05,
      "loss": 4.2442,
      "step": 102000
    },
    {
      "epoch": 1.3640567713774312,
      "grad_norm": 18.10131072998047,
      "learning_rate": 2.7265569203607344e-05,
      "loss": 4.5589,
      "step": 102500
    },
    {
      "epoch": 1.370710706847565,
      "grad_norm": 10.778820037841797,
      "learning_rate": 2.7154669541185916e-05,
      "loss": 4.423,
      "step": 103000
    },
    {
      "epoch": 1.3773646423176988,
      "grad_norm": 18.95659828186035,
      "learning_rate": 2.704376987876449e-05,
      "loss": 4.3836,
      "step": 103500
    },
    {
      "epoch": 1.3840185777878327,
      "grad_norm": 17.37736701965332,
      "learning_rate": 2.6932870216343066e-05,
      "loss": 4.2108,
      "step": 104000
    },
    {
      "epoch": 1.3906725132579665,
      "grad_norm": 14.620933532714844,
      "learning_rate": 2.6821970553921634e-05,
      "loss": 4.0505,
      "step": 104500
    },
    {
      "epoch": 1.3973264487281003,
      "grad_norm": 8.460728645324707,
      "learning_rate": 2.6711070891500205e-05,
      "loss": 4.2926,
      "step": 105000
    },
    {
      "epoch": 1.4039803841982341,
      "grad_norm": 10.959943771362305,
      "learning_rate": 2.660017122907878e-05,
      "loss": 4.045,
      "step": 105500
    },
    {
      "epoch": 1.4106343196683677,
      "grad_norm": 28.09206771850586,
      "learning_rate": 2.6489271566657352e-05,
      "loss": 4.1698,
      "step": 106000
    },
    {
      "epoch": 1.4172882551385015,
      "grad_norm": 16.2005672454834,
      "learning_rate": 2.6378371904235927e-05,
      "loss": 4.1973,
      "step": 106500
    },
    {
      "epoch": 1.4239421906086354,
      "grad_norm": 8.207967758178711,
      "learning_rate": 2.6267472241814495e-05,
      "loss": 4.259,
      "step": 107000
    },
    {
      "epoch": 1.4305961260787692,
      "grad_norm": 2.5860462188720703,
      "learning_rate": 2.6156572579393067e-05,
      "loss": 4.5243,
      "step": 107500
    },
    {
      "epoch": 1.437250061548903,
      "grad_norm": 5.606235980987549,
      "learning_rate": 2.604567291697164e-05,
      "loss": 4.2238,
      "step": 108000
    },
    {
      "epoch": 1.4439039970190368,
      "grad_norm": 11.878273963928223,
      "learning_rate": 2.5934773254550217e-05,
      "loss": 4.3346,
      "step": 108500
    },
    {
      "epoch": 1.4505579324891706,
      "grad_norm": 5.463265419006348,
      "learning_rate": 2.5823873592128788e-05,
      "loss": 4.1234,
      "step": 109000
    },
    {
      "epoch": 1.4572118679593045,
      "grad_norm": 11.184011459350586,
      "learning_rate": 2.5712973929707356e-05,
      "loss": 4.3516,
      "step": 109500
    },
    {
      "epoch": 1.4638658034294383,
      "grad_norm": 9.261282920837402,
      "learning_rate": 2.560207426728593e-05,
      "loss": 4.3983,
      "step": 110000
    },
    {
      "epoch": 1.470519738899572,
      "grad_norm": 15.879664421081543,
      "learning_rate": 2.5491174604864503e-05,
      "loss": 4.1661,
      "step": 110500
    },
    {
      "epoch": 1.477173674369706,
      "grad_norm": 9.033500671386719,
      "learning_rate": 2.5380274942443078e-05,
      "loss": 4.4285,
      "step": 111000
    },
    {
      "epoch": 1.4838276098398397,
      "grad_norm": 8.067781448364258,
      "learning_rate": 2.5269375280021653e-05,
      "loss": 4.0146,
      "step": 111500
    },
    {
      "epoch": 1.4904815453099736,
      "grad_norm": 6.379144668579102,
      "learning_rate": 2.5158475617600217e-05,
      "loss": 4.3945,
      "step": 112000
    },
    {
      "epoch": 1.4971354807801074,
      "grad_norm": 6.501380920410156,
      "learning_rate": 2.5047575955178792e-05,
      "loss": 4.2697,
      "step": 112500
    },
    {
      "epoch": 1.5037894162502412,
      "grad_norm": 7.290963649749756,
      "learning_rate": 2.4936676292757367e-05,
      "loss": 4.1189,
      "step": 113000
    },
    {
      "epoch": 1.510443351720375,
      "grad_norm": 11.32045841217041,
      "learning_rate": 2.482577663033594e-05,
      "loss": 4.0569,
      "step": 113500
    },
    {
      "epoch": 1.5170972871905088,
      "grad_norm": 9.132554054260254,
      "learning_rate": 2.471487696791451e-05,
      "loss": 4.0421,
      "step": 114000
    },
    {
      "epoch": 1.5237512226606427,
      "grad_norm": 0.0,
      "learning_rate": 2.4603977305493085e-05,
      "loss": 4.2942,
      "step": 114500
    },
    {
      "epoch": 1.5304051581307765,
      "grad_norm": 16.0106143951416,
      "learning_rate": 2.4493077643071654e-05,
      "loss": 4.3376,
      "step": 115000
    },
    {
      "epoch": 1.5370590936009103,
      "grad_norm": 5.910149574279785,
      "learning_rate": 2.438217798065023e-05,
      "loss": 4.1979,
      "step": 115500
    },
    {
      "epoch": 1.5437130290710441,
      "grad_norm": 15.902297973632812,
      "learning_rate": 2.42712783182288e-05,
      "loss": 4.5287,
      "step": 116000
    },
    {
      "epoch": 1.550366964541178,
      "grad_norm": 26.34619903564453,
      "learning_rate": 2.416037865580737e-05,
      "loss": 4.4184,
      "step": 116500
    },
    {
      "epoch": 1.5570209000113118,
      "grad_norm": 0.0,
      "learning_rate": 2.4049478993385947e-05,
      "loss": 4.4381,
      "step": 117000
    },
    {
      "epoch": 1.5636748354814456,
      "grad_norm": 13.469009399414062,
      "learning_rate": 2.3938579330964518e-05,
      "loss": 4.3717,
      "step": 117500
    },
    {
      "epoch": 1.5703287709515794,
      "grad_norm": 13.233156204223633,
      "learning_rate": 2.382767966854309e-05,
      "loss": 4.1886,
      "step": 118000
    },
    {
      "epoch": 1.5769827064217132,
      "grad_norm": 4.715651512145996,
      "learning_rate": 2.3716780006121665e-05,
      "loss": 4.4501,
      "step": 118500
    },
    {
      "epoch": 1.583636641891847,
      "grad_norm": 10.138016700744629,
      "learning_rate": 2.3605880343700236e-05,
      "loss": 4.3674,
      "step": 119000
    },
    {
      "epoch": 1.5902905773619809,
      "grad_norm": 0.0665518119931221,
      "learning_rate": 2.3494980681278808e-05,
      "loss": 4.4191,
      "step": 119500
    },
    {
      "epoch": 1.5969445128321147,
      "grad_norm": 8.72125244140625,
      "learning_rate": 2.338408101885738e-05,
      "loss": 4.1295,
      "step": 120000
    },
    {
      "epoch": 1.6035984483022485,
      "grad_norm": 11.476576805114746,
      "learning_rate": 2.327318135643595e-05,
      "loss": 4.1303,
      "step": 120500
    },
    {
      "epoch": 1.6102523837723823,
      "grad_norm": 12.68110466003418,
      "learning_rate": 2.3162281694014522e-05,
      "loss": 4.3117,
      "step": 121000
    },
    {
      "epoch": 1.616906319242516,
      "grad_norm": 8.238896369934082,
      "learning_rate": 2.3051382031593097e-05,
      "loss": 4.5103,
      "step": 121500
    },
    {
      "epoch": 1.6235602547126498,
      "grad_norm": 13.04244327545166,
      "learning_rate": 2.294048236917167e-05,
      "loss": 4.11,
      "step": 122000
    },
    {
      "epoch": 1.6302141901827836,
      "grad_norm": 3.6275618076324463,
      "learning_rate": 2.282958270675024e-05,
      "loss": 4.2252,
      "step": 122500
    },
    {
      "epoch": 1.6368681256529174,
      "grad_norm": 0.0,
      "learning_rate": 2.2718683044328815e-05,
      "loss": 4.4713,
      "step": 123000
    },
    {
      "epoch": 1.6435220611230512,
      "grad_norm": 25.891136169433594,
      "learning_rate": 2.2607783381907387e-05,
      "loss": 4.0936,
      "step": 123500
    },
    {
      "epoch": 1.650175996593185,
      "grad_norm": 6.151860237121582,
      "learning_rate": 2.249688371948596e-05,
      "loss": 4.1692,
      "step": 124000
    },
    {
      "epoch": 1.6568299320633189,
      "grad_norm": 6.319221019744873,
      "learning_rate": 2.2385984057064533e-05,
      "loss": 3.993,
      "step": 124500
    },
    {
      "epoch": 1.6634838675334527,
      "grad_norm": 8.170735359191895,
      "learning_rate": 2.22750843946431e-05,
      "loss": 4.0566,
      "step": 125000
    },
    {
      "epoch": 1.6701378030035865,
      "grad_norm": 11.806455612182617,
      "learning_rate": 2.2164184732221677e-05,
      "loss": 4.1181,
      "step": 125500
    },
    {
      "epoch": 1.6767917384737203,
      "grad_norm": 7.542710781097412,
      "learning_rate": 2.2053285069800248e-05,
      "loss": 4.5006,
      "step": 126000
    },
    {
      "epoch": 1.6834456739438541,
      "grad_norm": 7.708675384521484,
      "learning_rate": 2.194238540737882e-05,
      "loss": 4.2013,
      "step": 126500
    },
    {
      "epoch": 1.690099609413988,
      "grad_norm": 12.801871299743652,
      "learning_rate": 2.1831485744957395e-05,
      "loss": 4.7533,
      "step": 127000
    },
    {
      "epoch": 1.6967535448841216,
      "grad_norm": 15.463829040527344,
      "learning_rate": 2.1720586082535966e-05,
      "loss": 4.3159,
      "step": 127500
    },
    {
      "epoch": 1.7034074803542554,
      "grad_norm": 0.26106348633766174,
      "learning_rate": 2.1609686420114538e-05,
      "loss": 4.042,
      "step": 128000
    },
    {
      "epoch": 1.7100614158243892,
      "grad_norm": 7.568021774291992,
      "learning_rate": 2.149878675769311e-05,
      "loss": 4.4642,
      "step": 128500
    },
    {
      "epoch": 1.716715351294523,
      "grad_norm": 19.406641006469727,
      "learning_rate": 2.1387887095271684e-05,
      "loss": 4.1759,
      "step": 129000
    },
    {
      "epoch": 1.7233692867646568,
      "grad_norm": 10.199527740478516,
      "learning_rate": 2.1276987432850256e-05,
      "loss": 3.9408,
      "step": 129500
    },
    {
      "epoch": 1.7300232222347907,
      "grad_norm": 26.29263687133789,
      "learning_rate": 2.1166087770428827e-05,
      "loss": 4.43,
      "step": 130000
    },
    {
      "epoch": 1.7366771577049245,
      "grad_norm": 4.711642742156982,
      "learning_rate": 2.1055188108007402e-05,
      "loss": 3.999,
      "step": 130500
    },
    {
      "epoch": 1.7433310931750583,
      "grad_norm": 11.54320240020752,
      "learning_rate": 2.094428844558597e-05,
      "loss": 4.2168,
      "step": 131000
    },
    {
      "epoch": 1.7499850286451921,
      "grad_norm": 14.557608604431152,
      "learning_rate": 2.0833388783164545e-05,
      "loss": 4.3838,
      "step": 131500
    },
    {
      "epoch": 1.756638964115326,
      "grad_norm": 18.11019515991211,
      "learning_rate": 2.072248912074312e-05,
      "loss": 4.2882,
      "step": 132000
    },
    {
      "epoch": 1.7632928995854598,
      "grad_norm": 5.842344760894775,
      "learning_rate": 2.061158945832169e-05,
      "loss": 4.2066,
      "step": 132500
    },
    {
      "epoch": 1.7699468350555936,
      "grad_norm": 8.389156341552734,
      "learning_rate": 2.0500689795900263e-05,
      "loss": 4.206,
      "step": 133000
    },
    {
      "epoch": 1.7766007705257274,
      "grad_norm": 5.584636688232422,
      "learning_rate": 2.0389790133478835e-05,
      "loss": 4.0434,
      "step": 133500
    },
    {
      "epoch": 1.7832547059958612,
      "grad_norm": 8.160688400268555,
      "learning_rate": 2.0278890471057407e-05,
      "loss": 3.9798,
      "step": 134000
    },
    {
      "epoch": 1.789908641465995,
      "grad_norm": 17.41034698486328,
      "learning_rate": 2.0167990808635978e-05,
      "loss": 4.3992,
      "step": 134500
    },
    {
      "epoch": 1.7965625769361289,
      "grad_norm": 7.000288009643555,
      "learning_rate": 2.0057091146214553e-05,
      "loss": 4.44,
      "step": 135000
    },
    {
      "epoch": 1.8032165124062627,
      "grad_norm": 10.881917953491211,
      "learning_rate": 1.9946191483793125e-05,
      "loss": 4.3997,
      "step": 135500
    },
    {
      "epoch": 1.8098704478763965,
      "grad_norm": 19.661203384399414,
      "learning_rate": 1.9835291821371696e-05,
      "loss": 4.8543,
      "step": 136000
    },
    {
      "epoch": 1.8165243833465303,
      "grad_norm": 4.968430042266846,
      "learning_rate": 1.972439215895027e-05,
      "loss": 4.1498,
      "step": 136500
    },
    {
      "epoch": 1.8231783188166641,
      "grad_norm": 5.885617733001709,
      "learning_rate": 1.961349249652884e-05,
      "loss": 4.2212,
      "step": 137000
    },
    {
      "epoch": 1.829832254286798,
      "grad_norm": 15.007607460021973,
      "learning_rate": 1.9502592834107414e-05,
      "loss": 4.0664,
      "step": 137500
    },
    {
      "epoch": 1.8364861897569318,
      "grad_norm": 8.771001815795898,
      "learning_rate": 1.9391693171685986e-05,
      "loss": 4.3978,
      "step": 138000
    },
    {
      "epoch": 1.8431401252270656,
      "grad_norm": 5.851621627807617,
      "learning_rate": 1.9280793509264557e-05,
      "loss": 4.2286,
      "step": 138500
    },
    {
      "epoch": 1.8497940606971994,
      "grad_norm": 14.479124069213867,
      "learning_rate": 1.9169893846843132e-05,
      "loss": 4.2828,
      "step": 139000
    },
    {
      "epoch": 1.8564479961673332,
      "grad_norm": 15.182619094848633,
      "learning_rate": 1.9058994184421704e-05,
      "loss": 4.2019,
      "step": 139500
    },
    {
      "epoch": 1.863101931637467,
      "grad_norm": 7.373623847961426,
      "learning_rate": 1.8948094522000275e-05,
      "loss": 4.3182,
      "step": 140000
    },
    {
      "epoch": 1.8697558671076009,
      "grad_norm": 37.48945236206055,
      "learning_rate": 1.883719485957885e-05,
      "loss": 4.1291,
      "step": 140500
    },
    {
      "epoch": 1.8764098025777347,
      "grad_norm": 10.526424407958984,
      "learning_rate": 1.872629519715742e-05,
      "loss": 4.5096,
      "step": 141000
    },
    {
      "epoch": 1.8830637380478685,
      "grad_norm": 13.76383113861084,
      "learning_rate": 1.8615395534735993e-05,
      "loss": 4.237,
      "step": 141500
    },
    {
      "epoch": 1.8897176735180023,
      "grad_norm": 7.411828517913818,
      "learning_rate": 1.8504495872314565e-05,
      "loss": 4.1769,
      "step": 142000
    },
    {
      "epoch": 1.8963716089881362,
      "grad_norm": 6.172264575958252,
      "learning_rate": 1.8393596209893137e-05,
      "loss": 4.1601,
      "step": 142500
    },
    {
      "epoch": 1.90302554445827,
      "grad_norm": 29.505996704101562,
      "learning_rate": 1.828269654747171e-05,
      "loss": 3.9976,
      "step": 143000
    },
    {
      "epoch": 1.9096794799284038,
      "grad_norm": 10.434504508972168,
      "learning_rate": 1.8171796885050283e-05,
      "loss": 4.1089,
      "step": 143500
    },
    {
      "epoch": 1.9163334153985374,
      "grad_norm": 11.013653755187988,
      "learning_rate": 1.8060897222628855e-05,
      "loss": 4.9708,
      "step": 144000
    },
    {
      "epoch": 1.9229873508686712,
      "grad_norm": 6.411275863647461,
      "learning_rate": 1.7949997560207426e-05,
      "loss": 4.1765,
      "step": 144500
    },
    {
      "epoch": 1.929641286338805,
      "grad_norm": 6.031689643859863,
      "learning_rate": 1.7839097897786e-05,
      "loss": 4.5552,
      "step": 145000
    },
    {
      "epoch": 1.9362952218089389,
      "grad_norm": 14.206026077270508,
      "learning_rate": 1.7728198235364573e-05,
      "loss": 4.4158,
      "step": 145500
    },
    {
      "epoch": 1.9429491572790727,
      "grad_norm": 25.24947166442871,
      "learning_rate": 1.7617298572943144e-05,
      "loss": 4.0835,
      "step": 146000
    },
    {
      "epoch": 1.9496030927492065,
      "grad_norm": 7.771877288818359,
      "learning_rate": 1.750639891052172e-05,
      "loss": 4.2343,
      "step": 146500
    },
    {
      "epoch": 1.9562570282193403,
      "grad_norm": 6.624374866485596,
      "learning_rate": 1.7395499248100287e-05,
      "loss": 4.0587,
      "step": 147000
    },
    {
      "epoch": 1.9629109636894742,
      "grad_norm": 0.6701995730400085,
      "learning_rate": 1.7284599585678862e-05,
      "loss": 4.0277,
      "step": 147500
    },
    {
      "epoch": 1.969564899159608,
      "grad_norm": 0.0,
      "learning_rate": 1.7173699923257434e-05,
      "loss": 4.4264,
      "step": 148000
    },
    {
      "epoch": 1.9762188346297418,
      "grad_norm": 8.600595474243164,
      "learning_rate": 1.7062800260836005e-05,
      "loss": 4.0805,
      "step": 148500
    },
    {
      "epoch": 1.9828727700998756,
      "grad_norm": 1.7325708866119385,
      "learning_rate": 1.695190059841458e-05,
      "loss": 4.2321,
      "step": 149000
    },
    {
      "epoch": 1.9895267055700094,
      "grad_norm": 0.0,
      "learning_rate": 1.6841000935993152e-05,
      "loss": 4.2719,
      "step": 149500
    },
    {
      "epoch": 1.996180641040143,
      "grad_norm": 4.863222122192383,
      "learning_rate": 1.6730101273571724e-05,
      "loss": 4.1004,
      "step": 150000
    },
    {
      "epoch": 2.002834576510277,
      "grad_norm": 16.076757431030273,
      "learning_rate": 1.6619201611150295e-05,
      "loss": 4.114,
      "step": 150500
    },
    {
      "epoch": 2.0094885119804107,
      "grad_norm": 6.837696075439453,
      "learning_rate": 1.650830194872887e-05,
      "loss": 3.7613,
      "step": 151000
    },
    {
      "epoch": 2.0161424474505445,
      "grad_norm": 11.769864082336426,
      "learning_rate": 1.639740228630744e-05,
      "loss": 3.7304,
      "step": 151500
    },
    {
      "epoch": 2.0227963829206783,
      "grad_norm": 11.543401718139648,
      "learning_rate": 1.6286502623886013e-05,
      "loss": 3.7553,
      "step": 152000
    },
    {
      "epoch": 2.029450318390812,
      "grad_norm": 12.252175331115723,
      "learning_rate": 1.6175602961464588e-05,
      "loss": 3.8869,
      "step": 152500
    },
    {
      "epoch": 2.036104253860946,
      "grad_norm": 11.537841796875,
      "learning_rate": 1.6064703299043156e-05,
      "loss": 3.8762,
      "step": 153000
    },
    {
      "epoch": 2.0427581893310798,
      "grad_norm": 0.12775219976902008,
      "learning_rate": 1.595380363662173e-05,
      "loss": 4.1616,
      "step": 153500
    },
    {
      "epoch": 2.0494121248012136,
      "grad_norm": 8.390140533447266,
      "learning_rate": 1.5842903974200303e-05,
      "loss": 3.6712,
      "step": 154000
    },
    {
      "epoch": 2.0560660602713474,
      "grad_norm": 9.889077186584473,
      "learning_rate": 1.5732004311778874e-05,
      "loss": 3.799,
      "step": 154500
    },
    {
      "epoch": 2.0627199957414812,
      "grad_norm": 4.5421366691589355,
      "learning_rate": 1.562110464935745e-05,
      "loss": 4.162,
      "step": 155000
    },
    {
      "epoch": 2.069373931211615,
      "grad_norm": 13.496797561645508,
      "learning_rate": 1.551020498693602e-05,
      "loss": 3.8883,
      "step": 155500
    },
    {
      "epoch": 2.076027866681749,
      "grad_norm": 6.545027732849121,
      "learning_rate": 1.5399305324514592e-05,
      "loss": 3.9191,
      "step": 156000
    },
    {
      "epoch": 2.0826818021518827,
      "grad_norm": 0.0,
      "learning_rate": 1.5288405662093167e-05,
      "loss": 3.5544,
      "step": 156500
    },
    {
      "epoch": 2.0893357376220165,
      "grad_norm": 11.470596313476562,
      "learning_rate": 1.5177505999671737e-05,
      "loss": 3.8059,
      "step": 157000
    },
    {
      "epoch": 2.0959896730921503,
      "grad_norm": 6.879572868347168,
      "learning_rate": 1.506660633725031e-05,
      "loss": 4.0667,
      "step": 157500
    },
    {
      "epoch": 2.102643608562284,
      "grad_norm": 5.664806842803955,
      "learning_rate": 1.4955706674828882e-05,
      "loss": 3.9238,
      "step": 158000
    },
    {
      "epoch": 2.109297544032418,
      "grad_norm": 11.687315940856934,
      "learning_rate": 1.4844807012407455e-05,
      "loss": 3.6027,
      "step": 158500
    },
    {
      "epoch": 2.115951479502552,
      "grad_norm": 10.676653861999512,
      "learning_rate": 1.4733907349986028e-05,
      "loss": 3.7229,
      "step": 159000
    },
    {
      "epoch": 2.1226054149726856,
      "grad_norm": 9.689518928527832,
      "learning_rate": 1.46230076875646e-05,
      "loss": 3.8934,
      "step": 159500
    },
    {
      "epoch": 2.1292593504428194,
      "grad_norm": 2.854092597961426,
      "learning_rate": 1.4512108025143173e-05,
      "loss": 3.8315,
      "step": 160000
    },
    {
      "epoch": 2.1359132859129533,
      "grad_norm": 20.80794906616211,
      "learning_rate": 1.4401208362721743e-05,
      "loss": 3.7762,
      "step": 160500
    },
    {
      "epoch": 2.142567221383087,
      "grad_norm": 7.373933792114258,
      "learning_rate": 1.4290308700300316e-05,
      "loss": 3.8507,
      "step": 161000
    },
    {
      "epoch": 2.149221156853221,
      "grad_norm": 20.336463928222656,
      "learning_rate": 1.4179409037878891e-05,
      "loss": 3.7062,
      "step": 161500
    },
    {
      "epoch": 2.1558750923233547,
      "grad_norm": 87.14884948730469,
      "learning_rate": 1.4068509375457461e-05,
      "loss": 3.7136,
      "step": 162000
    },
    {
      "epoch": 2.1625290277934885,
      "grad_norm": 6.612425327301025,
      "learning_rate": 1.3957609713036034e-05,
      "loss": 3.6387,
      "step": 162500
    },
    {
      "epoch": 2.1691829632636224,
      "grad_norm": 8.07975959777832,
      "learning_rate": 1.3846710050614606e-05,
      "loss": 3.8336,
      "step": 163000
    },
    {
      "epoch": 2.175836898733756,
      "grad_norm": 10.483236312866211,
      "learning_rate": 1.373581038819318e-05,
      "loss": 3.6247,
      "step": 163500
    },
    {
      "epoch": 2.18249083420389,
      "grad_norm": 0.0,
      "learning_rate": 1.362491072577175e-05,
      "loss": 4.1461,
      "step": 164000
    },
    {
      "epoch": 2.189144769674024,
      "grad_norm": 6.197422504425049,
      "learning_rate": 1.3514011063350324e-05,
      "loss": 3.961,
      "step": 164500
    },
    {
      "epoch": 2.1957987051441576,
      "grad_norm": 5.986391067504883,
      "learning_rate": 1.3403111400928897e-05,
      "loss": 3.6805,
      "step": 165000
    },
    {
      "epoch": 2.2024526406142915,
      "grad_norm": 8.242199897766113,
      "learning_rate": 1.3292211738507467e-05,
      "loss": 4.0529,
      "step": 165500
    },
    {
      "epoch": 2.2091065760844253,
      "grad_norm": 8.564041137695312,
      "learning_rate": 1.3181312076086042e-05,
      "loss": 3.9075,
      "step": 166000
    },
    {
      "epoch": 2.215760511554559,
      "grad_norm": 6.79123592376709,
      "learning_rate": 1.3070412413664612e-05,
      "loss": 3.974,
      "step": 166500
    },
    {
      "epoch": 2.222414447024693,
      "grad_norm": 6.5439934730529785,
      "learning_rate": 1.2959512751243185e-05,
      "loss": 4.1043,
      "step": 167000
    },
    {
      "epoch": 2.2290683824948267,
      "grad_norm": 0.0,
      "learning_rate": 1.2848613088821758e-05,
      "loss": 3.8461,
      "step": 167500
    },
    {
      "epoch": 2.2357223179649606,
      "grad_norm": 6.8951873779296875,
      "learning_rate": 1.273771342640033e-05,
      "loss": 3.8795,
      "step": 168000
    },
    {
      "epoch": 2.2423762534350944,
      "grad_norm": 7.613293170928955,
      "learning_rate": 1.2626813763978903e-05,
      "loss": 3.985,
      "step": 168500
    },
    {
      "epoch": 2.2490301889052278,
      "grad_norm": 10.67851448059082,
      "learning_rate": 1.2515914101557475e-05,
      "loss": 3.6876,
      "step": 169000
    },
    {
      "epoch": 2.255684124375362,
      "grad_norm": 11.670812606811523,
      "learning_rate": 1.2405014439136048e-05,
      "loss": 3.8907,
      "step": 169500
    },
    {
      "epoch": 2.2623380598454954,
      "grad_norm": 12.795103073120117,
      "learning_rate": 1.229411477671462e-05,
      "loss": 3.8936,
      "step": 170000
    },
    {
      "epoch": 2.2689919953156297,
      "grad_norm": 22.789051055908203,
      "learning_rate": 1.2183215114293193e-05,
      "loss": 3.7725,
      "step": 170500
    },
    {
      "epoch": 2.275645930785763,
      "grad_norm": 4.257595539093018,
      "learning_rate": 1.2072315451871766e-05,
      "loss": 3.8258,
      "step": 171000
    },
    {
      "epoch": 2.282299866255897,
      "grad_norm": 10.892552375793457,
      "learning_rate": 1.1961415789450338e-05,
      "loss": 4.0232,
      "step": 171500
    },
    {
      "epoch": 2.2889538017260307,
      "grad_norm": 0.02397867478430271,
      "learning_rate": 1.185051612702891e-05,
      "loss": 3.8775,
      "step": 172000
    },
    {
      "epoch": 2.2956077371961645,
      "grad_norm": 6.117353439331055,
      "learning_rate": 1.1739616464607483e-05,
      "loss": 3.7557,
      "step": 172500
    },
    {
      "epoch": 2.3022616726662983,
      "grad_norm": 7.697844982147217,
      "learning_rate": 1.1628716802186054e-05,
      "loss": 3.7577,
      "step": 173000
    },
    {
      "epoch": 2.308915608136432,
      "grad_norm": 11.25182819366455,
      "learning_rate": 1.1517817139764626e-05,
      "loss": 3.887,
      "step": 173500
    },
    {
      "epoch": 2.315569543606566,
      "grad_norm": 8.337048530578613,
      "learning_rate": 1.14069174773432e-05,
      "loss": 3.8924,
      "step": 174000
    },
    {
      "epoch": 2.3222234790767,
      "grad_norm": 9.494153022766113,
      "learning_rate": 1.1296017814921772e-05,
      "loss": 4.0719,
      "step": 174500
    },
    {
      "epoch": 2.3288774145468336,
      "grad_norm": 5.882181644439697,
      "learning_rate": 1.1185118152500344e-05,
      "loss": 3.935,
      "step": 175000
    },
    {
      "epoch": 2.3355313500169674,
      "grad_norm": 7.133645057678223,
      "learning_rate": 1.1074218490078917e-05,
      "loss": 3.6515,
      "step": 175500
    },
    {
      "epoch": 2.3421852854871013,
      "grad_norm": 11.241351127624512,
      "learning_rate": 1.0963318827657488e-05,
      "loss": 3.8373,
      "step": 176000
    },
    {
      "epoch": 2.348839220957235,
      "grad_norm": 14.629376411437988,
      "learning_rate": 1.0852419165236062e-05,
      "loss": 3.9762,
      "step": 176500
    },
    {
      "epoch": 2.355493156427369,
      "grad_norm": 9.964665412902832,
      "learning_rate": 1.0741519502814635e-05,
      "loss": 3.7402,
      "step": 177000
    },
    {
      "epoch": 2.3621470918975027,
      "grad_norm": 12.406052589416504,
      "learning_rate": 1.0630619840393207e-05,
      "loss": 3.558,
      "step": 177500
    },
    {
      "epoch": 2.3688010273676365,
      "grad_norm": 0.0,
      "learning_rate": 1.0519720177971778e-05,
      "loss": 4.2101,
      "step": 178000
    },
    {
      "epoch": 2.3754549628377704,
      "grad_norm": 5.023351669311523,
      "learning_rate": 1.0408820515550351e-05,
      "loss": 3.9537,
      "step": 178500
    },
    {
      "epoch": 2.382108898307904,
      "grad_norm": 16.27779769897461,
      "learning_rate": 1.0297920853128925e-05,
      "loss": 3.9301,
      "step": 179000
    },
    {
      "epoch": 2.388762833778038,
      "grad_norm": 9.810490608215332,
      "learning_rate": 1.0187021190707496e-05,
      "loss": 3.5307,
      "step": 179500
    },
    {
      "epoch": 2.395416769248172,
      "grad_norm": 7.659199237823486,
      "learning_rate": 1.0076121528286068e-05,
      "loss": 4.1001,
      "step": 180000
    },
    {
      "epoch": 2.4020707047183056,
      "grad_norm": 0.0,
      "learning_rate": 9.965221865864641e-06,
      "loss": 4.0479,
      "step": 180500
    },
    {
      "epoch": 2.4087246401884395,
      "grad_norm": 37.87263107299805,
      "learning_rate": 9.854322203443213e-06,
      "loss": 4.0477,
      "step": 181000
    },
    {
      "epoch": 2.4153785756585733,
      "grad_norm": 10.541178703308105,
      "learning_rate": 9.743422541021786e-06,
      "loss": 3.9675,
      "step": 181500
    },
    {
      "epoch": 2.422032511128707,
      "grad_norm": 8.140848159790039,
      "learning_rate": 9.632522878600359e-06,
      "loss": 3.7993,
      "step": 182000
    },
    {
      "epoch": 2.428686446598841,
      "grad_norm": 6.076809883117676,
      "learning_rate": 9.52162321617893e-06,
      "loss": 4.2419,
      "step": 182500
    },
    {
      "epoch": 2.4353403820689747,
      "grad_norm": 7.6108503341674805,
      "learning_rate": 9.410723553757502e-06,
      "loss": 3.7618,
      "step": 183000
    },
    {
      "epoch": 2.4419943175391086,
      "grad_norm": 7.019556045532227,
      "learning_rate": 9.299823891336075e-06,
      "loss": 3.8983,
      "step": 183500
    },
    {
      "epoch": 2.4486482530092424,
      "grad_norm": 12.475302696228027,
      "learning_rate": 9.188924228914647e-06,
      "loss": 3.5747,
      "step": 184000
    },
    {
      "epoch": 2.455302188479376,
      "grad_norm": 5.403421878814697,
      "learning_rate": 9.07802456649322e-06,
      "loss": 3.7035,
      "step": 184500
    },
    {
      "epoch": 2.46195612394951,
      "grad_norm": 13.345255851745605,
      "learning_rate": 8.967124904071793e-06,
      "loss": 3.8032,
      "step": 185000
    },
    {
      "epoch": 2.468610059419644,
      "grad_norm": 17.333650588989258,
      "learning_rate": 8.856225241650365e-06,
      "loss": 3.9318,
      "step": 185500
    },
    {
      "epoch": 2.4752639948897777,
      "grad_norm": 10.673318862915039,
      "learning_rate": 8.745325579228937e-06,
      "loss": 3.7197,
      "step": 186000
    },
    {
      "epoch": 2.4819179303599115,
      "grad_norm": 13.354341506958008,
      "learning_rate": 8.63442591680751e-06,
      "loss": 3.9002,
      "step": 186500
    },
    {
      "epoch": 2.4885718658300453,
      "grad_norm": 13.558095932006836,
      "learning_rate": 8.523526254386081e-06,
      "loss": 3.7855,
      "step": 187000
    },
    {
      "epoch": 2.495225801300179,
      "grad_norm": 12.425824165344238,
      "learning_rate": 8.412626591964655e-06,
      "loss": 3.6644,
      "step": 187500
    },
    {
      "epoch": 2.501879736770313,
      "grad_norm": 14.548279762268066,
      "learning_rate": 8.301726929543228e-06,
      "loss": 3.6295,
      "step": 188000
    },
    {
      "epoch": 2.5085336722404468,
      "grad_norm": 5.538427829742432,
      "learning_rate": 8.1908272671218e-06,
      "loss": 3.6945,
      "step": 188500
    },
    {
      "epoch": 2.5151876077105806,
      "grad_norm": 12.378745079040527,
      "learning_rate": 8.079927604700371e-06,
      "loss": 3.6972,
      "step": 189000
    },
    {
      "epoch": 2.521841543180714,
      "grad_norm": 8.164094924926758,
      "learning_rate": 7.969027942278944e-06,
      "loss": 3.7753,
      "step": 189500
    },
    {
      "epoch": 2.5284954786508482,
      "grad_norm": 11.140680313110352,
      "learning_rate": 7.858128279857517e-06,
      "loss": 4.3157,
      "step": 190000
    },
    {
      "epoch": 2.5351494141209816,
      "grad_norm": 0.0,
      "learning_rate": 7.747228617436089e-06,
      "loss": 4.0499,
      "step": 190500
    },
    {
      "epoch": 2.541803349591116,
      "grad_norm": 9.127949714660645,
      "learning_rate": 7.63632895501466e-06,
      "loss": 3.7217,
      "step": 191000
    },
    {
      "epoch": 2.5484572850612492,
      "grad_norm": 0.0,
      "learning_rate": 7.525429292593234e-06,
      "loss": 4.0206,
      "step": 191500
    },
    {
      "epoch": 2.5551112205313835,
      "grad_norm": 9.595115661621094,
      "learning_rate": 7.414529630171805e-06,
      "loss": 4.045,
      "step": 192000
    },
    {
      "epoch": 2.561765156001517,
      "grad_norm": 6.058917999267578,
      "learning_rate": 7.3036299677503795e-06,
      "loss": 3.8523,
      "step": 192500
    },
    {
      "epoch": 2.568419091471651,
      "grad_norm": 5.232116222381592,
      "learning_rate": 7.192730305328951e-06,
      "loss": 3.7707,
      "step": 193000
    },
    {
      "epoch": 2.5750730269417845,
      "grad_norm": 0.027239099144935608,
      "learning_rate": 7.0818306429075235e-06,
      "loss": 3.9725,
      "step": 193500
    },
    {
      "epoch": 2.581726962411919,
      "grad_norm": 7.77354097366333,
      "learning_rate": 6.970930980486096e-06,
      "loss": 3.8345,
      "step": 194000
    },
    {
      "epoch": 2.588380897882052,
      "grad_norm": 13.559941291809082,
      "learning_rate": 6.8600313180646674e-06,
      "loss": 3.8136,
      "step": 194500
    },
    {
      "epoch": 2.595034833352186,
      "grad_norm": 18.67658042907715,
      "learning_rate": 6.74913165564324e-06,
      "loss": 3.885,
      "step": 195000
    },
    {
      "epoch": 2.60168876882232,
      "grad_norm": 1.3012959957122803,
      "learning_rate": 6.638231993221813e-06,
      "loss": 3.6801,
      "step": 195500
    },
    {
      "epoch": 2.6083427042924536,
      "grad_norm": 5.870944499969482,
      "learning_rate": 6.5273323308003855e-06,
      "loss": 3.7524,
      "step": 196000
    },
    {
      "epoch": 2.6149966397625874,
      "grad_norm": 20.67557716369629,
      "learning_rate": 6.416432668378958e-06,
      "loss": 3.6842,
      "step": 196500
    },
    {
      "epoch": 2.6216505752327213,
      "grad_norm": 6.0514020919799805,
      "learning_rate": 6.30553300595753e-06,
      "loss": 3.7816,
      "step": 197000
    },
    {
      "epoch": 2.628304510702855,
      "grad_norm": 15.520438194274902,
      "learning_rate": 6.194633343536103e-06,
      "loss": 3.9198,
      "step": 197500
    },
    {
      "epoch": 2.634958446172989,
      "grad_norm": 8.340729713439941,
      "learning_rate": 6.083733681114675e-06,
      "loss": 3.59,
      "step": 198000
    },
    {
      "epoch": 2.6416123816431227,
      "grad_norm": 13.44694709777832,
      "learning_rate": 5.9728340186932475e-06,
      "loss": 3.8453,
      "step": 198500
    },
    {
      "epoch": 2.6482663171132566,
      "grad_norm": 6.9727911949157715,
      "learning_rate": 5.86193435627182e-06,
      "loss": 3.8447,
      "step": 199000
    },
    {
      "epoch": 2.6549202525833904,
      "grad_norm": 9.685012817382812,
      "learning_rate": 5.751034693850392e-06,
      "loss": 3.8565,
      "step": 199500
    },
    {
      "epoch": 2.661574188053524,
      "grad_norm": 10.829485893249512,
      "learning_rate": 5.640135031428965e-06,
      "loss": 3.8295,
      "step": 200000
    },
    {
      "epoch": 2.668228123523658,
      "grad_norm": 22.117265701293945,
      "learning_rate": 5.529235369007537e-06,
      "loss": 3.7776,
      "step": 200500
    },
    {
      "epoch": 2.674882058993792,
      "grad_norm": 14.798768997192383,
      "learning_rate": 5.4183357065861095e-06,
      "loss": 3.8417,
      "step": 201000
    },
    {
      "epoch": 2.6815359944639257,
      "grad_norm": 0.05190226063132286,
      "learning_rate": 5.307436044164682e-06,
      "loss": 3.7176,
      "step": 201500
    },
    {
      "epoch": 2.6881899299340595,
      "grad_norm": 11.713340759277344,
      "learning_rate": 5.196536381743254e-06,
      "loss": 3.5756,
      "step": 202000
    },
    {
      "epoch": 2.6948438654041933,
      "grad_norm": 0.0,
      "learning_rate": 5.085636719321826e-06,
      "loss": 3.9535,
      "step": 202500
    },
    {
      "epoch": 2.701497800874327,
      "grad_norm": 2.293095588684082,
      "learning_rate": 4.974737056900399e-06,
      "loss": 3.6735,
      "step": 203000
    },
    {
      "epoch": 2.708151736344461,
      "grad_norm": 5.136761665344238,
      "learning_rate": 4.8638373944789715e-06,
      "loss": 3.7493,
      "step": 203500
    },
    {
      "epoch": 2.7148056718145948,
      "grad_norm": 11.630702018737793,
      "learning_rate": 4.752937732057544e-06,
      "loss": 3.9113,
      "step": 204000
    },
    {
      "epoch": 2.7214596072847286,
      "grad_norm": 7.4404802322387695,
      "learning_rate": 4.642038069636116e-06,
      "loss": 3.9309,
      "step": 204500
    },
    {
      "epoch": 2.7281135427548624,
      "grad_norm": 12.883838653564453,
      "learning_rate": 4.531138407214689e-06,
      "loss": 3.6172,
      "step": 205000
    },
    {
      "epoch": 2.734767478224996,
      "grad_norm": 12.14793586730957,
      "learning_rate": 4.420238744793261e-06,
      "loss": 3.7744,
      "step": 205500
    },
    {
      "epoch": 2.74142141369513,
      "grad_norm": 12.057661056518555,
      "learning_rate": 4.3093390823718336e-06,
      "loss": 4.1492,
      "step": 206000
    },
    {
      "epoch": 2.748075349165264,
      "grad_norm": 5.7796244621276855,
      "learning_rate": 4.198439419950405e-06,
      "loss": 3.9948,
      "step": 206500
    },
    {
      "epoch": 2.7547292846353977,
      "grad_norm": 7.664506435394287,
      "learning_rate": 4.087539757528978e-06,
      "loss": 3.723,
      "step": 207000
    },
    {
      "epoch": 2.7613832201055315,
      "grad_norm": 6.989078044891357,
      "learning_rate": 3.976640095107551e-06,
      "loss": 3.8096,
      "step": 207500
    },
    {
      "epoch": 2.7680371555756653,
      "grad_norm": 0.10478822141885757,
      "learning_rate": 3.865740432686123e-06,
      "loss": 3.676,
      "step": 208000
    },
    {
      "epoch": 2.774691091045799,
      "grad_norm": 1.4043296575546265,
      "learning_rate": 3.7548407702646956e-06,
      "loss": 3.918,
      "step": 208500
    },
    {
      "epoch": 2.781345026515933,
      "grad_norm": 7.419092178344727,
      "learning_rate": 3.6439411078432676e-06,
      "loss": 3.7219,
      "step": 209000
    },
    {
      "epoch": 2.787998961986067,
      "grad_norm": 13.084269523620605,
      "learning_rate": 3.5330414454218404e-06,
      "loss": 3.7076,
      "step": 209500
    },
    {
      "epoch": 2.7946528974562006,
      "grad_norm": 15.14905834197998,
      "learning_rate": 3.422141783000413e-06,
      "loss": 3.8929,
      "step": 210000
    },
    {
      "epoch": 2.8013068329263344,
      "grad_norm": 0.0,
      "learning_rate": 3.3112421205789848e-06,
      "loss": 4.0097,
      "step": 210500
    },
    {
      "epoch": 2.8079607683964682,
      "grad_norm": 9.410945892333984,
      "learning_rate": 3.2003424581575576e-06,
      "loss": 3.8463,
      "step": 211000
    },
    {
      "epoch": 2.814614703866602,
      "grad_norm": 4.218315601348877,
      "learning_rate": 3.08944279573613e-06,
      "loss": 3.7813,
      "step": 211500
    },
    {
      "epoch": 2.8212686393367354,
      "grad_norm": 10.610374450683594,
      "learning_rate": 2.9785431333147024e-06,
      "loss": 4.3494,
      "step": 212000
    },
    {
      "epoch": 2.8279225748068697,
      "grad_norm": 10.136178016662598,
      "learning_rate": 2.867643470893275e-06,
      "loss": 3.8976,
      "step": 212500
    },
    {
      "epoch": 2.834576510277003,
      "grad_norm": 7.430744647979736,
      "learning_rate": 2.7567438084718472e-06,
      "loss": 3.7076,
      "step": 213000
    },
    {
      "epoch": 2.8412304457471373,
      "grad_norm": 6.7498908042907715,
      "learning_rate": 2.6458441460504196e-06,
      "loss": 3.636,
      "step": 213500
    },
    {
      "epoch": 2.8478843812172707,
      "grad_norm": 7.377382278442383,
      "learning_rate": 2.534944483628992e-06,
      "loss": 3.6126,
      "step": 214000
    },
    {
      "epoch": 2.854538316687405,
      "grad_norm": 0.0,
      "learning_rate": 2.4240448212075644e-06,
      "loss": 3.8977,
      "step": 214500
    },
    {
      "epoch": 2.8611922521575384,
      "grad_norm": 11.037739753723145,
      "learning_rate": 2.313145158786137e-06,
      "loss": 3.8274,
      "step": 215000
    },
    {
      "epoch": 2.8678461876276726,
      "grad_norm": 7.2798357009887695,
      "learning_rate": 2.2022454963647092e-06,
      "loss": 3.8126,
      "step": 215500
    },
    {
      "epoch": 2.874500123097806,
      "grad_norm": 5.869070053100586,
      "learning_rate": 2.0913458339432816e-06,
      "loss": 3.8405,
      "step": 216000
    },
    {
      "epoch": 2.8811540585679403,
      "grad_norm": 10.470690727233887,
      "learning_rate": 1.980446171521854e-06,
      "loss": 3.9247,
      "step": 216500
    },
    {
      "epoch": 2.8878079940380736,
      "grad_norm": 0.03860272467136383,
      "learning_rate": 1.8695465091004264e-06,
      "loss": 3.5268,
      "step": 217000
    },
    {
      "epoch": 2.8944619295082075,
      "grad_norm": 2.3612587451934814,
      "learning_rate": 1.7586468466789986e-06,
      "loss": 4.0932,
      "step": 217500
    },
    {
      "epoch": 2.9011158649783413,
      "grad_norm": 6.577132225036621,
      "learning_rate": 1.6477471842575713e-06,
      "loss": 3.7508,
      "step": 218000
    },
    {
      "epoch": 2.907769800448475,
      "grad_norm": 25.538301467895508,
      "learning_rate": 1.5368475218361437e-06,
      "loss": 4.0174,
      "step": 218500
    },
    {
      "epoch": 2.914423735918609,
      "grad_norm": 26.24858283996582,
      "learning_rate": 1.425947859414716e-06,
      "loss": 3.9862,
      "step": 219000
    },
    {
      "epoch": 2.9210776713887427,
      "grad_norm": 11.321674346923828,
      "learning_rate": 1.3150481969932885e-06,
      "loss": 3.7679,
      "step": 219500
    },
    {
      "epoch": 2.9277316068588766,
      "grad_norm": 18.302627563476562,
      "learning_rate": 1.2041485345718609e-06,
      "loss": 3.8492,
      "step": 220000
    },
    {
      "epoch": 2.9343855423290104,
      "grad_norm": 7.48773193359375,
      "learning_rate": 1.0932488721504333e-06,
      "loss": 3.6171,
      "step": 220500
    },
    {
      "epoch": 2.941039477799144,
      "grad_norm": 9.514081001281738,
      "learning_rate": 9.823492097290057e-07,
      "loss": 3.727,
      "step": 221000
    },
    {
      "epoch": 2.947693413269278,
      "grad_norm": 4.552280902862549,
      "learning_rate": 8.71449547307578e-07,
      "loss": 3.5027,
      "step": 221500
    },
    {
      "epoch": 2.954347348739412,
      "grad_norm": 16.222036361694336,
      "learning_rate": 7.605498848861505e-07,
      "loss": 3.9543,
      "step": 222000
    },
    {
      "epoch": 2.9610012842095457,
      "grad_norm": 12.868552207946777,
      "learning_rate": 6.496502224647229e-07,
      "loss": 3.6284,
      "step": 222500
    },
    {
      "epoch": 2.9676552196796795,
      "grad_norm": 8.490703582763672,
      "learning_rate": 5.387505600432953e-07,
      "loss": 3.6088,
      "step": 223000
    },
    {
      "epoch": 2.9743091551498133,
      "grad_norm": 6.374440670013428,
      "learning_rate": 4.278508976218677e-07,
      "loss": 3.8859,
      "step": 223500
    },
    {
      "epoch": 2.980963090619947,
      "grad_norm": 6.549242973327637,
      "learning_rate": 3.169512352004401e-07,
      "loss": 3.9607,
      "step": 224000
    },
    {
      "epoch": 2.987617026090081,
      "grad_norm": 10.122856140136719,
      "learning_rate": 2.0605157277901248e-07,
      "loss": 3.8789,
      "step": 224500
    },
    {
      "epoch": 2.9942709615602148,
      "grad_norm": 9.272760391235352,
      "learning_rate": 9.515191035758488e-08,
      "loss": 4.0215,
      "step": 225000
    }
  ],
  "logging_steps": 500,
  "max_steps": 225429,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.9451400740864e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
